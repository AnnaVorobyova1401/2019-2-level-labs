Knowledge Representation
For the semantic's web-to-function, computers must
have access to structured collections of information
and sets of inference rules that they can use to
conduct automated reasoning. Artificial-intelligence
researchers have studied such systems since long
before the Web was developed. Knowledge
representation, as this technology is often called, is
currently in a state comparable to that of hypertext
before the advent of the Web: it is clearly a good
idea, and some very nice demonstrations exist, but
it has not yet changed the world. It contains the
seeds of important applications, but to realize its full
potential it must be linked into a single global
system.
BY MIGUEL SALMERON
WEB SEARCHES TODAY
Traditional
knowledgerepresentation
systems typically
have been
centralized,
requiring everyone
to share exactly
the same definition
of common
concepts such as
"parent" or
"vehicle." But
central control is stifling, and increasing the size and
scope of such a system rapidly becomes
unmanageable.
Moreover, these systems usually carefully limit the
questions that can be asked so that the computer
can answer reliably— or answer at all. The problem
is reminiscent of Godel's theorem from mathematics:
Scientific American: The Semantic Web
any system that is complex enough to be useful also
encompasses unanswerable questions, much like
sophisticated versions of the basic paradox "This
sentence is false." To avoid such problems,
traditional knowledge-representation systems
generally each had their own narrow and
idiosyncratic set of rules for making inferences
about their data. For example, a genealogy system,
acting on a database of family trees, might include
the rule "a wife of an uncle is an aunt." Even if the
data could be transferred from one system to
another, the rules, existing in a completely different
form, usually could not.
Semantic Web researchers, in contrast, accept that
paradoxes and unanswerable questions are a price
that must be paid to achieve versatility. We make
the language for the rules as expressive as needed
to allow the Web to reason as widely as desired.
This philosophy is similar to that of the conventional
Web: early in the Web's development, detractors
pointed out that it could never be a well-organized
library; without a central database and tree
structure, one would never be sure of finding
everything. They were right. But the expressive
power of the system made vast amounts of
information available, and search engines (which
would have seemed quite impractical a decade ago)
now produce remarkably complete indices of a lot of
the material out there. The challenge of the
Semantic Web, therefore, is to provide a language
that expresses both data and rules for reasoning
about the data and that allows rules from any
existing knowledge-representation system to be
exported onto the Web.
Adding logic to the Web—the means to use rules to
make inferences, choose courses of action and
answer questions—is the task before the Semantic
Web community at the moment. A mixture of
mathematical and engineering decisions complicate
this task. The logic must be powerful enough to
describe complex properties of objects but not so
powerful that agents can be tricked by being asked
to consider a paradox. Fortunately, a large majority
of the information we want to express is along the
lines of "a hex-head bolt is a type of machine bolt,"
which is readily written in existing languages with a
little extra vocabulary.
Two important technologies for developing the
Semantic Web are already in place: eXtensible
Markup Language (XML) and the Resource
Description Framework (RDF). XML lets everyone
create their own tags—hidden labels such as or that
annotate Web pages or sections of text on a page.
Scripts, or programs, can make use of these tags in
sophisticated ways, but the script writer has to know
what the page writer uses each tag for. In short,
XML allows users to add arbitrary structure to their
documents but says nothing about what the
structures mean.
The Semantic Web will enable machines to
COMPREHEND semantic documents and data,
not human speech and writings.
Meaning is expressed by RDF, which encodes it in
sets of triples, each triple being rather like the
subject, verb and object of an elementary sentence.
These triples can be written using XML tags. In
RDF, a document makes assertions that particular
things (people, Web pages or whatever) have
properties (such as "is a sister of," "is the author of")
with certain values (another person, another Web
page). This structure turns out to be a natural way to
describe the vast majority of the data processed by
machines. Subject and object are each identified by
a Universal Resource Identifier (URI), just as used
in a link on a Web page. (URLs, Uniform Resource
Locators, are the most common type of URI.) The
verbs are also identified by URIs, which enables
anyone to define a new concept, a new verb, just by
defining a URI for it somewhere on the Web.
Human language thrives when using the same term
to mean somewhat different things, but automation
does not. Imagine that I hire a clown messenger
service to deliver balloons to my customers on their
birthdays. Unfortunately, the service transfers the
addresses from my database to its database, not
knowing that the "addresses" in mine are where bills
are sent and that many of them are post office
boxes. My hired clowns end up entertaining a
number of postal workers—not necessarily a bad
thing but certainly not the intended effect. Using a
different URI for each specific concept solves that
problem. An address that is a mailing address can
be distinguished from one that is a street address,
and both can be distinguished from an address that
is a speech.
The triples of RDF form webs of information about
related things. Because RDF uses URIs to encode
this information in a document, the URIs ensure that
concepts are not just words in a document but are
tied to a unique definition that everyone can find on
the Web. For example, imagine that we have access
to a variety of databases with information about
people, including their addresses. If we want to find
people living in a specific zip code, we need to know
which fields in each database represent names and
which represent zip codes. RDF can specify that
"(field 5 in database A) (is a field of type) (zip
code)," using URIs rather than phrases for each
term.
Ontologies
Of course, this is not the end of the story, because
two databases may use different identifiers for what
is in fact the same concept, such as zip code. A
program that wants to compare or combine
information across the two databases has to know
that these two terms are being used to mean the
same thing. Ideally, the program must have a way to
discover such common meanings for whatever
databases it encounters.
A solution to this problem is provided by the third
basic component of the Semantic Web, collections
of information called ontologies. In philosophy, an
ontology is a theory about the nature of existence, of
what types of things exist; ontology as a discipline
studies such theories. Artificial-intelligence and Web
researchers have co-opted the term for their own
jargon, and for them an ontology is a document or
file that formally defines the relations among terms.
The most typical kind of ontology for the Web has a
taxonomy and a set of inference rules.
The taxonomy defines classes of objects and
relations among them. For example, an address
may be defined as a type of location, and city codes
may be defined to apply only to locations, and so
on. Classes, subclasses and relations among
entities are a very powerful tool for Web use. We
can express a large number of relations among
entities by assigning properties to classes and
allowing subclasses to inherit such properties. If city
codes must be of type city and cities generally have
Web sites, we can discuss the Web site associated
with a city code even if no database links a city code
directly to a Web site.
Inference rules in ontologies supply further power.
An ontology may express the rule "If a city code is
associated with a state code, and an address uses
that city code, then that address has the associated
state code." A program could then readily deduce,
for instance, that a Cornell University address, being
in Ithaca, must be in New York State, which is in the
U.S., and therefore should be formatted to U.S.
standards. The computer doesn't truly "understand"
any of this information, but it can now manipulate
the terms much more effectively in ways that are
useful and meaningful to the human user.
With ontology pages on the Web, solutions to
terminology (and other) problems begin to emerge.
The meaning of terms or XML codes used on a Web
page can be defined by pointers from the page to an
ontology. Of course, the same problems as before
now arise if I point to an ontology that defines
addresses as containing a zip code and you point to
one that uses postal code. This kind of confusion
can be resolved if ontologies (or other Web
services) provide equivalence relations: one or both
of our ontologies may contain the information that
my zip code is equivalent to your postal code.
Our scheme for sending in the clowns to entertain
my customers is partially solved when the two
databases point to different definitions of address.
The program, using distinct URIs for different
concepts of address, will not confuse them and in
fact will need to discover that the concepts are
related at all. The program could then use a service
that takes a list of postal addresses (defined in the
first ontology) and converts it into a list of physical
addresses (the second ontology) by recognizing and
removing post office boxes and other unsuitable
addresses. The structure and semantics provided by
ontologies make it easier for an entrepreneur to
provide such a service and can make its use
completely transparent.
Ontologies can enhance the functioning of the Web
in many ways. They can be used in a simple fashion
to improve the accuracy of Web searches—the
search program can look for only those pages that
refer to a precise concept instead of all the ones
using ambiguous keywords. More advanced
applications will use ontologies to relate the
information on a page to the associated knowledge
structures and inference rules. An example of a
page marked up for such use is online at
http://www.cs.umd.edu/~hendler. If you send your
Web browser to that page, you will see the normal
Web page entitled "Dr. James A. Hendler." As a
human, you can readily find the link to a short
biographical note and read there that Hendler
received his Ph.D. from Brown University. A
computer program trying to find such information,
however, would have to be very complex to guess
that this information might be in a biography and to
understand the English language used there.
For computers, the page is linked to an ontology
page that defines information about computer
science departments. For instance, professors work
at universities and they generally have doctorates.
Further markup on the page (not displayed by the
typical Web browser) uses the ontology's concepts
to specify that Hendler received his Ph.D. from the
entity described at the URI http://www. brown.edu —
the Web page for Brown. Computers can also find
that Hendler is a member of a particular research
project, has a particular e-mail address, and so on.
All that information is readily processed by a
computer and could be used to answer queries
(such as where Dr. Hendler received his degree)
that currently would require a human to sift through
the content of various pages turned up by a search
engine.
In addition, this markup makes it much easier to
develop programs that can tackle complicated
questions whose answers do not reside on a single
Web page. Suppose you wish to find the Ms. Cook
you met at a trade conference last year. You don't
remember her first name, but you remember that
she worked for one of your clients and that her son
was a student at your alma mater. An intelligent
search program can sift through all the pages of
people whose name is "Cook" (sidestepping all the
pages relating to cooks, cooking, the Cook Islands
and so forth), find the ones that mention working for
a company that's on your list of clients and follow
links to Web pages of their children to track down if
any are in school at the right place.
